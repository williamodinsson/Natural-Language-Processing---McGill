# Natural-Language-Processing---McGill

Week 1 

Working with the two example codes, either with example data such as the book downloaded
from Gutenberg in the examples or with text data of particular interest to you, please answer
the following questions briefly.
1. What data did you use? If it is a book from Gutenberg, which one?
2. Can you estimate how many sentences it contains? What can you do to compute this
(conceptually, in Python and/or in R)?
3. How about paragraphs? Can you estimate how many there are? What can you do to
compute this (conceptually, in Python and/or in R)?
4. How many names (of places and people) are mentioned in the text? What did you do to
compute this (conceptually, in Python and/or in R)?
5. What are the ten most frequent words in the text that you consider to clearly be stop
words?
6. What are the ten most frequent words in the text that you consider to not be stop words?
7. How would you go about automatically identifying potential stop words for documents
written in a language you do not speak?
8. Looking at a histogram of the word frequencies (like the horizontal bar chart in the R
example code), what can be said about the shape of the distribution?
9. Please show an example of a word cloud you created from your text along with a code
snippet of how you did it.
You may censure any potentially confidential terms that appear in your data if you are
using workplace experiential learning and the data set you used is not in open data.
Also feel free to comment on any insights you have gained of the data while carrying out
these exercises as well as your initial feeling on the usefulness of Python and R in NLP.

